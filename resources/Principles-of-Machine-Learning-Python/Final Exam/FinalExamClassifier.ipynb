{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principles of Machine Learning - Final Exam - Classifier\n",
    "\n",
    "The task of the classifier is predict the likeliness that a potential new customer will buy a bike from our company as well.\n",
    "\n",
    "Our data foundation is a set of 16,400 previous customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn import linear_model\n",
    "import sklearn.metrics as sklm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "restored = pd.read_csv(\"ExportedFeatures.csv\")\n",
    "restored_buyers = pd.read_csv(\"ExportedBuyers.csv\")\n",
    "restored_spend = pd.read_csv(\"ExportedSpend.csv\")\n",
    "restored_test = pd.read_csv(\"ExportedTest.csv\")\n",
    "Features = restored.to_numpy()\n",
    "buyers = restored_buyers.to_numpy()\n",
    "spend = restored_spend.to_numpy()\n",
    "Test_Features = restored_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10990, 26) (5414, 26) (10990, 1) (5414, 1)\n",
      "[[0.82757764 0.17242236]\n",
      " [0.98277459 0.01722541]\n",
      " [0.9475091  0.0524909 ]\n",
      " [0.56180937 0.43819063]\n",
      " [0.31168059 0.68831941]\n",
      " [0.81032178 0.18967822]\n",
      " [0.74249378 0.25750622]\n",
      " [0.15346552 0.84653448]\n",
      " [0.91924619 0.08075381]\n",
      " [0.4240567  0.5759433 ]\n",
      " [0.73519914 0.26480086]\n",
      " [0.95500394 0.04499606]\n",
      " [0.70839169 0.29160831]\n",
      " [0.84329305 0.15670695]\n",
      " [0.88648519 0.11351481]]\n",
      "                 Confusion matrix\n",
      "                 Score positive    Score negative\n",
      "Actual positive      3283               377\n",
      "Actual negative       785               969\n",
      "\n",
      "Accuracy  0.79\n",
      " \n",
      "           Positive      Negative\n",
      "Num case     3660          1754\n",
      "Precision    0.81          0.72\n",
      "Recall       0.90          0.55\n",
      "F1           0.85          0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael Ikemann\\Anaconda3\\envs\\lechler_ims\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Michael Ikemann\\Anaconda3\\envs\\lechler_ims\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "labels = np.array(buyers)\n",
    "# ohe = preprocessing.OneHotEncoder(categories='auto')\n",
    "# encoded = ohe.fit(labels.reshape(-1,1))\n",
    "# labels = encoded.transform(labels.reshape(-1,1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = ms.train_test_split(Features, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "print(\"{} {} {} {}\".format(X_train.shape, X_test.shape, y_train.shape, y_test.shape))\n",
    "\n",
    "logistic_mod = linear_model.LogisticRegression(max_iter=100) \n",
    "logistic_mod.fit(X_train, y_train)\n",
    "\n",
    "def print_metrics(labels, scores):\n",
    "    metrics = sklm.precision_recall_fscore_support(labels, scores)\n",
    "    conf = sklm.confusion_matrix(labels, scores)\n",
    "    print('                 Confusion matrix')\n",
    "    print('                 Score positive    Score negative')\n",
    "    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n",
    "    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n",
    "    print('')\n",
    "    print('Accuracy  %0.2f' % sklm.accuracy_score(labels, scores))\n",
    "    print(' ')\n",
    "    print('           Positive      Negative')\n",
    "    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n",
    "    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n",
    "    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n",
    "    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\n",
    "\n",
    "\n",
    "def score_model(probs, threshold):\n",
    "    return np.array([1 if x > threshold else 0 for x in probs[:,1]])\n",
    "\n",
    "probabilities = logistic_mod.predict_proba(X_test)\n",
    "print(probabilities[:15,:])\n",
    "scores = score_model(probabilities, 0.5)    \n",
    "print_metrics(y_test, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael Ikemann\\Anaconda3\\envs\\lechler_ims\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Confusion matrix\n",
      "                 Score positive    Score negative\n",
      "Actual positive      3221               439\n",
      "Actual negative       698              1056\n",
      "\n",
      "Accuracy  0.79\n",
      " \n",
      "           Positive      Negative\n",
      "Num case     3660          1754\n",
      "Precision    0.82          0.71\n",
      "Recall       0.88          0.60\n",
      "F1           0.85          0.65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ab_clf = AdaBoostClassifier(learning_rate=1.0, n_estimators=500)\n",
    "ab_clf.fit(X_train, y_train)\n",
    "\n",
    "probabilities = ab_clf.predict_proba(X_test)\n",
    "scores = score_model(probabilities, 0.5)    \n",
    "print_metrics(y_test, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "Train on 10990 samples, validate on 5414 samples\n",
      "Epoch 1/20\n",
      "10990/10990 [==============================] - 2s 160us/step - loss: 0.6501 - acc: 0.7818 - val_loss: 0.4787 - val_acc: 0.7772\n",
      "Epoch 2/20\n",
      "10990/10990 [==============================] - 1s 122us/step - loss: 0.4652 - acc: 0.7848 - val_loss: 0.4677 - val_acc: 0.7830\n",
      "Epoch 3/20\n",
      "10990/10990 [==============================] - 1s 122us/step - loss: 0.4622 - acc: 0.7830 - val_loss: 0.4694 - val_acc: 0.7857\n",
      "Epoch 4/20\n",
      "10990/10990 [==============================] - 1s 122us/step - loss: 0.4630 - acc: 0.7862 - val_loss: 0.4746 - val_acc: 0.7789\n",
      "Epoch 5/20\n",
      "10990/10990 [==============================] - 1s 124us/step - loss: 0.4635 - acc: 0.7818 - val_loss: 0.4671 - val_acc: 0.7837\n",
      "Epoch 6/20\n",
      "10990/10990 [==============================] - 1s 124us/step - loss: 0.4599 - acc: 0.7857 - val_loss: 0.4678 - val_acc: 0.7819\n",
      "Epoch 7/20\n",
      "10990/10990 [==============================] - 1s 125us/step - loss: 0.4618 - acc: 0.7833 - val_loss: 0.4642 - val_acc: 0.7835\n",
      "Epoch 8/20\n",
      "10990/10990 [==============================] - 1s 125us/step - loss: 0.4604 - acc: 0.7855 - val_loss: 0.4710 - val_acc: 0.7800\n",
      "Epoch 9/20\n",
      "10990/10990 [==============================] - 1s 125us/step - loss: 0.4605 - acc: 0.7874 - val_loss: 0.4713 - val_acc: 0.7785\n",
      "Epoch 10/20\n",
      "10990/10990 [==============================] - 1s 125us/step - loss: 0.4594 - acc: 0.7859 - val_loss: 0.4677 - val_acc: 0.7808\n",
      "Epoch 11/20\n",
      "10990/10990 [==============================] - 1s 125us/step - loss: 0.4600 - acc: 0.7842 - val_loss: 0.4642 - val_acc: 0.7826\n",
      "Epoch 12/20\n",
      "10990/10990 [==============================] - 1s 124us/step - loss: 0.4585 - acc: 0.7852 - val_loss: 0.4650 - val_acc: 0.7833\n",
      "Epoch 13/20\n",
      "10990/10990 [==============================] - 1s 126us/step - loss: 0.4589 - acc: 0.7855 - val_loss: 0.4689 - val_acc: 0.7824\n",
      "Epoch 14/20\n",
      "10990/10990 [==============================] - 1s 125us/step - loss: 0.4599 - acc: 0.7867 - val_loss: 0.4636 - val_acc: 0.7843\n",
      "Epoch 15/20\n",
      "10990/10990 [==============================] - 1s 123us/step - loss: 0.4586 - acc: 0.7862 - val_loss: 0.4642 - val_acc: 0.7813\n",
      "Epoch 16/20\n",
      "10990/10990 [==============================] - 1s 124us/step - loss: 0.4585 - acc: 0.7846 - val_loss: 0.4654 - val_acc: 0.7817\n",
      "Epoch 17/20\n",
      "10990/10990 [==============================] - 1s 124us/step - loss: 0.4581 - acc: 0.7852 - val_loss: 0.4640 - val_acc: 0.7856\n",
      "Epoch 18/20\n",
      "10990/10990 [==============================] - 1s 124us/step - loss: 0.4594 - acc: 0.7844 - val_loss: 0.4642 - val_acc: 0.7824\n",
      "Epoch 19/20\n",
      "10990/10990 [==============================] - 1s 123us/step - loss: 0.4599 - acc: 0.7822 - val_loss: 0.4634 - val_acc: 0.7846\n",
      "Epoch 20/20\n",
      "10990/10990 [==============================] - 1s 123us/step - loss: 0.4579 - acc: 0.7851 - val_loss: 0.4658 - val_acc: 0.7850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x297f05525f8>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.regularizers import L1L2\n",
    "\n",
    "ohe = preprocessing.OneHotEncoder(categories='auto', sparse=False)\n",
    "encoded = ohe.fit(y_train.reshape(-1,1))\n",
    "yn_train = encoded.transform(y_train.reshape(-1,1))\n",
    "yn_test = encoded.transform(y_test.reshape(-1,1))\n",
    "\n",
    "print(y_test[:5])\n",
    "print(yn_test[:5])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100))\n",
    "model.add(Dense(100))\n",
    "model.add(Dense(100))\n",
    "model.add(Dense(100))\n",
    "model.add(Dense(100))\n",
    "model.add(Dense(2,activation='softmax', kernel_regularizer=L1L2(l1=0.0, l2=0.2),input_dim=X_train.shape[1]))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, yn_train, epochs=20, validation_data=(X_test, yn_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 1 1 1 0 0 0 0 0 0]\n",
      "(500,)\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "probabilities = logistic_mod.predict_proba(Test_Features)\n",
    "scores = score_model(probabilities, 0.5)\n",
    "print(scores[:15])\n",
    "print(scores.shape)\n",
    "\n",
    "for score in scores:\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
